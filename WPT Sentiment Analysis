{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1858576,"sourceType":"datasetVersion","datasetId":1105807},{"sourceId":2304311,"sourceType":"datasetVersion","datasetId":1389694},{"sourceId":4547702,"sourceType":"datasetVersion","datasetId":2655259},{"sourceId":8958593,"sourceType":"datasetVersion","datasetId":5391899}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import train_test_split\nfrom gensim.models import FastText\nfrom transformers import BertTokenizer, TFBertModel, TFBertForSequenceClassification\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Dropout, Concatenate, Embedding\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport fasttext.util","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disney = pd.read_csv('/kaggle/input/disneyland-reviews/DisneylandReviews.csv', encoding='latin-1')\nuniversal = pd.read_csv('/kaggle/input/reviewuniversalstudio/universal_studio_branches.csv')\nmuseum = pd.read_csv('/kaggle/input/trip-advisor-review-british-museum-in-london/Data_Review_British_Museum.csv')\n\nnewquay = pd.read_csv('/kaggle/input/wild-planet-trust-review-data/Newquay Reviews.csv')\nnewquay1 = pd.read_excel('/kaggle/input/wild-planet-trust-review-data/Newquay Reviews.xlsx')\n\npaignton = pd.read_csv('/kaggle/input/wild-planet-trust-review-data/Paignton Reviews.csv')\npaignton1 = pd.read_excel('/kaggle/input/wild-planet-trust-review-data/Paignton Reviews.xlsx')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disney.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disney.drop(columns=['Review_ID', 'Branch', 'Reviewer_Location'], inplace=True)\ndisney.rename(columns={'Year_Month': 'written_date', 'Review_Text': 'review', 'Rating': 'rating'}, inplace=True)\ndisney.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"universal.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"universal['review'] = universal.title + \" \" + universal.review_text\nuniversal.drop(columns=['reviewer', 'branch', 'title', 'review_text'], inplace=True)\nuniversal.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"museum.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"museum['review'] = museum.title + \" \" + museum.comment\nmuseum.drop(columns=['title', 'comment', 'trip', 'writer'], inplace=True)\nmuseum.rename(columns={'written': 'written_date'}, inplace=True)\nmuseum.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"newquay.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"newquay.drop(columns=['reviewer'], inplace=True)\nnewquay.rename(columns={'review-date': 'written_date'}, inplace=True)\nnewquay.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"newquay1.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"newquay1.rename(columns={'text': 'review', 'stars':'rating', 'publishedAtDate': 'written_date'}, inplace=True)\nnewquay1.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"newquay = pd.concat([newquay, newquay1], ignore_index=True)\nnewquay","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"newquay = newquay.dropna(subset=['review'])\nnewquay = newquay[newquay['review'] != '']\nnewquay","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"paignton.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"paignton.drop(columns=['reviewer'], inplace=True)\npaignton.rename(columns={'review-date': 'written_date'}, inplace=True)\npaignton.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"paignton1.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"paignton1.rename(columns={'text': 'review', 'stars':'rating', 'publishedAtDate': 'written_date'}, inplace=True)\npaignton1.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"paignton = pd.concat([paignton, paignton1], ignore_index=True)\npaignton","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"paignton = paignton.dropna(subset=['review'])\npaignton = paignton[paignton['review'] != '']\npaignton","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # FastText embeddings\n# fasttext.util.download_model('en', if_exists='ignore')\n# ft = fasttext.load_model('cc.en.300.bin')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the FastText model from .vec file\nfasttext_model_path = '/kaggle/input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'\nft = KeyedVectors.load_word2vec_format(fasttext_model_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Download stopwords\nnltk.download('stopwords')\nstop_words = set(stopwords.words('english'))\n\ndef preprocess_text(text):\n    text = text.lower()\n    text = re.sub(r'\\d+', '', text)\n    text = re.sub(r'[^\\w\\s]', '', text)\n    text = re.sub(r'\\s+', ' ', text).strip()\n    text = ' '.join([word for word in text.split() if word not in stop_words])\n    return text\n\nfor df in [disney, universal, museum, paignton, newquay]:\n    df['cleaned_review'] = df['review'].apply(preprocess_text)\n    df['sentiment'] = df['rating'].apply(lambda x: 1 if x >= 4 else 0)\n    \ndef get_embedding(text, ft_model):\n    words = text.split()\n    word_vecs = [ft_model.get_word_vector(word) for word in words]\n    return np.mean(word_vecs, axis=0)\n\n# Preprocess reviews\ndef preprocess_reviews(df, ft_model):\n    df['review_embedding'] = df['review'].apply(lambda x: get_embedding(str(x), ft_model))\n    return df\n\n# Extract features and labels\ndef extract_features_labels(df):\n    X = np.stack(df['review_embedding'].values)\n    y = df['sentiment'].values\n    return X, y\n\n# # Split datasets into train and test sets\n# def split_data(df):\n#     X_train, X_test, y_train, y_test = train_test_split(df['cleaned_review'], df['sentiment'], test_size=0.2, random_state=42)\n#     return X_train, X_test, y_train, y_test\n\n# X_train1, X_test1, y_train1, y_test1 = split_data(disney)\n# X_train2, X_test2, y_train2, y_test2 = split_data(universal)\n# X_train3, X_test3, y_train3, y_test3 = split_data(museum)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disney = preprocess_reviews(disney, ft)\nuniversal = preprocess_reviews(universal, ft)\nmuseum = preprocess_reviews(museum, ft)\npaignton = preprocess_reviews(paignton, ft)\nnewquay = preprocess_reviews(newquay, ft)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_disney, y_disney = extract_features_labels(disney)\nX_universal, y_universal = extract_features_labels(universal)\nX_museum, y_museum = extract_features_labels(museum)\nX_paignton, y_paignton = extract_features_labels(paignton)\nX_newquay, y_newquay = extract_features_labels(newquay)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a custom dataset class\nclass ReviewDataset(Dataset):\n    def __init__(self, reviews, sentiments):\n        self.reviews = reviews\n        self.sentiments = sentiments\n\n    def __len__(self):\n        return len(self.reviews)\n\n    def __getitem__(self, idx):\n        return torch.tensor(self.reviews[idx], dtype=torch.float32), torch.tensor(self.sentiments[idx], dtype=torch.float32)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disney_dataset = ReviewDataset(X_disney, y_disney)\nuniversal_dataset = ReviewDataset(X_universal, y_universal)\nmuseum_dataset = ReviewDataset(X_museum, y_museum)\npaignton_dataset = ReviewDataset(X_paignton, y_paignton)\nnewquay_dataset = ReviewDataset(X_newquay, y_newquay)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Weighted Random Sampler\nweights = [0.3 / len(disney_dataset), 0.3 / len(universal_dataset), 0.4 / len(museum_dataset)]\nsampler = WeightedRandomSampler(weights=[weights[0]] * len(disney_dataset) + \n                                       [weights[1]] * len(universal_dataset) + \n                                       [weights[2]] * len(museum_dataset), \n                                num_samples=len(disney_dataset) + len(universal_dataset) + len(museum_dataset), replacement=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Combine datasets\ncombined_dataset = torch.utils.data.ConcatDataset([disney_dataset, universal_dataset, museum_dataset])\ncombined_loader = DataLoader(combined_dataset, batch_size=32, sampler=sampler, shuffle=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the model\nclass ANN(nn.Module):\n    def __init__(self):\n        super(ANN, self).__init__()\n        self.layer1 = nn.Linear(300, 128)\n        self.layer2 = nn.Linear(128, 64)\n        self.layer3 = nn.Linear(64, 32)\n        self.layer4 = nn.Linear(32, 1)\n        self.relu = nn.ReLU()\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        x = self.relu(self.layer1(x))\n        x = self.relu(self.layer2(x))\n        x = self.relu(self.layer3(x))\n        x = self.sigmoid(self.layer4(x))\n        return x","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize the model, loss function and optimizer\nmodel = ANN()\ncriterion = nn.BCELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training the model\nnum_epochs = 10\nfor epoch in range(num_epochs):\n    for reviews, sentiments in combined_loader:\n        optimizer.zero_grad()\n        outputs = model(reviews).squeeze()\n        loss = criterion(outputs, sentiments)\n        loss.backward()\n        optimizer.step()\n    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluation\ndef evaluate(model, dataset):\n    loader = DataLoader(dataset, batch_size=32, shuffle=False)\n    model.eval()\n    predictions, actuals = [], []\n    with torch.no_grad():\n        for reviews, sentiments in loader:\n            outputs = model(reviews).squeeze()\n            predicted_labels = (outputs > 0.5).float()\n            predictions.extend(predicted_labels.numpy())\n            actuals.extend(sentiments.numpy())\n    return np.array(predictions), np.array(actuals)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_paignton, actual_paignton = evaluate(model, paignton_dataset)\npred_newquay, actual_newquay = evaluate(model, newquay_dataset)\n\n# Print evaluation results\nprint(\"Paignton Predictions:\", pred_paignton)\nprint(\"Paignton Actuals:\", actual_paignton)\nprint(\"Newquay Predictions:\", pred_newquay)\nprint(\"Newquay Actuals:\", actual_newquay)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train FastText model\nfasttext_model = FastText(sentences=pd.concat([X_train1, X_train2, X_train3]).tolist(), window=5, min_count=5, workers=4)\n\n# Function to generate FastText embeddings\ndef generate_fasttext_embeddings(text_data):\n    embeddings = []\n    for text in text_data:\n        embedding = [fasttext_model.wv[word] for word in text.split() if word in fasttext_model.wv]\n        if embedding:\n            embeddings.append(np.mean(embedding, axis=0))\n        else:\n            embeddings.append(np.zeros(100))  # Handle out-of-vocabulary words\n    return np.array(embeddings)\n\n# Generate FastText embeddings for train and test data\nX_train1_embeddings = generate_fasttext_embeddings(X_train1)\nX_test1_embeddings = generate_fasttext_embeddings(X_test1)\n\nX_train2_embeddings = generate_fasttext_embeddings(X_train2)\nX_test2_embeddings = generate_fasttext_embeddings(X_test2)\n\nX_train3_embeddings = generate_fasttext_embeddings(X_train3)\nX_test3_embeddings = generate_fasttext_embeddings(X_test3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmax_length = 128\n\n# Function to tokenize and encode text data\ndef tokenize_and_encode(text_data):\n    return tokenizer(text_data.tolist(), padding='max_length', truncation=True, max_length=max_length, return_tensors='tf')\n\n# Tokenize and encode train and test data\nX_train1_encoded = tokenize_and_encode(X_train1)\nX_test1_encoded = tokenize_and_encode(X_test1)\n\nX_train2_encoded = tokenize_and_encode(X_train2)\nX_test2_encoded = tokenize_and_encode(X_test2)\n\nX_train3_encoded = tokenize_and_encode(X_train3)\nX_test3_encoded = tokenize_and_encode(X_test3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train1_encoded","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming you have precomputed FastText embeddings\nembedding_dim = 300  # Adjust this according to your FastText embedding dimensions\n\n# Define inputs\ninput_ids = Input(shape=(128,), dtype=tf.int32, name='input_ids')\ntoken_type_ids = Input(shape=(128,), dtype=tf.int32, name='token_type_ids')\nattention_mask = Input(shape=(128,), dtype=tf.int32, name='attention_mask')\n\n# Embedding layer (assuming precomputed FastText embeddings)\nembedding_layer = Embedding(input_dim=100, output_dim=embedding_dim, trainable=True)\nembeddings = embedding_layer(input_ids)\n\n# Concatenate embeddings with token_type_ids and attention_mask\nx = Concatenate()([embeddings, tf.expand_dims(tf.cast(token_type_ids, tf.float32), axis=-1), tf.expand_dims(tf.cast(attention_mask, tf.float32), axis=-1)])\n\n# Flatten or use GlobalAveragePooling1D to prepare for dense layers\nx = Flatten()(x)\n\n# ANN layers\nx = Dense(256, activation='relu')(x)\nx = Dense(128, activation='relu')(x)\nx = Dense(64, activation='relu')(x)\n\n# Output layer for binary classification with sigmoid activation\noutputs = Dense(1, activation='sigmoid')(x)\n\n# Define the model\nmodel = Model(inputs=[input_ids, token_type_ids, attention_mask], outputs=outputs)\n\n# Compile the model\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\n# Assuming you have three weighted datasets and corresponding test sets\ntrain_weights = [0.3, 0.3, 0.4]\n\n# Assuming train_1, train_2, train_3 are your weighted training datasets\n# train_1, train_2, train_3 should be tuples of (input_ids, token_type_ids, attention_mask, labels)\n\n# Concatenate all input_ids, token_type_ids, attention_mask, and labels\nall_input_ids = tf.concat([X_train1_encoded[0], X_train2_encoded[0], X_train3_encoded[0]], axis=0)\nall_token_type_ids = tf.concat([X_train1_encoded[1], X_train2_encoded[1], X_train3_encoded[1]], axis=0)\nall_attention_mask = tf.concat([X_train1_encoded[2], X_train2_encoded[2], X_train3_encoded[2]], axis=0)\nall_labels = tf.concat([X_train1_encoded[3], X_train2_encoded[3], X_train3_encoded[3]], axis=0)\n\n# Train the model on all datasets with respective weights\nhistory = model.fit([all_input_ids, all_token_type_ids, all_attention_mask], all_labels,\n                    epochs=10,\n                    batch_size=32,\n                    sample_weight=[train_weights[0] * tf.ones_like(X_train1_encoded[0][:, 0]),\n                                   train_weights[1] * tf.ones_like(X_train2_encoded[0][:, 0]),\n                                   train_weights[2] * tf.ones_like(X_train3_encoded[0][:, 0])])\n\n# Evaluate on test datasets\ntest_results_1 = model.evaluate([X_test1_encoding[0], X_test1_encoding[1], X_test1_encoding[2]], X_test1_encoding[3])\ntest_results_2 = model.evaluate([X_test2_encoding[0], X_test2_encoding[1], X_test2_encoding[2]], X_test2_encoding[3])\ntest_results_3 = model.evaluate([X_test3_encoding[0], X_test3_encoding[1], X_test3_encoding[2]], X_test3_encoding[3])\n\nprint(\"Test results for test_1:\")\nprint(f\"Loss: {test_results_1[0]}, Accuracy: {test_results_1[1]}\")\n\nprint(\"\\nTest results for test_2:\")\nprint(f\"Loss: {test_results_2[0]}, Accuracy: {test_results_2[1]}\")\n\nprint(\"\\nTest results for test_3:\")\nprint(f\"Loss: {test_results_3[0]}, Accuracy: {test_results_3[1]}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to create the model\ndef create_model():\n    # Define the BERT model\n    bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n\n    # Define input layers\n    input_ids = Input(shape=(100,), dtype=tf.int32, name='input_ids')\n#     attention_mask = Input(shape=(128,), dtype=tf.int32, name='attention_mask')\n    fasttext_embeddings = Input(shape=(100,), dtype=tf.float32, name='fasttext_embeddings')\n\n    # BERT embeddings\n    bert_output = bert_model(input_ids)[0]\n    \n    # Dropout some samples randomly\n    dropout = Dropout(0.1)(bert_output)\n\n    # Concatenate BERT embeddings with FastText embeddings\n    concatenated_features = Concatenate()([dropout, fasttext_embeddings])\n\n    # Define model architecture\n    dense = Dense(128, activation='relu')(concatenated_features)\n    dropout = Dropout(0.1)(dense)\n    output = Dense(3, activation='softmax')(dropout)\n    \n    model = Model(inputs=[input_ids, fasttext_embeddings], outputs=output)\n    return model\n\n# Define model\nmodel = create_model()\n\n# Define early stopping callback\nearly_stopping = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n\n# Compile model\noptimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\nmodel.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Training dataset weights\ntrain_weights = np.concatenate([\n    np.full(len(train_1), 0.3),\n    np.full(len(train_2), 0.3),\n    np.full(len(train_3), 0.4)\n])\n\n# Combine the training datasets\nX_train_input_ids = np.concatenate([train_1_input_ids, train_2_input_ids, train_3_input_ids])\nX_train_fasttext = np.concatenate([train_1_fasttext, train_2_fasttext, train_3_fasttext])\ny_train = np.concatenate([train_1_labels, train_2_labels, train_3_labels])\n\n# Train model\nhistory = model.fit(\n    [X_train_input_ids, X_train_fasttext], \n    to_categorical(y_train, num_classes=3), \n    sample_weight=train_weights,\n    epochs=7, \n    batch_size=32, \n    validation_split=0.2, \n    callbacks=[early_stopping]\n)\n\n# Combine the test datasets\nX_test_input_ids = np.concatenate([test_1_input_ids, test_2_input_ids, test_3_input_ids])\nX_test_fasttext = np.concatenate([test_1_fasttext, test_2_fasttext, test_3_fasttext])\ny_test = np.concatenate([test_1_labels, test_2_labels, test_3_labels])\n\n# Evaluate model\nloss, accuracy = model.evaluate(\n    [X_test_input_ids, X_test_fasttext], \n    to_categorical(y_test, num_classes=3)\n)\nprint(f'Test Loss: {loss}, Test Accuracy: {accuracy}')\n\n# Predict and evaluate performance\ny_prob_bert = model.predict([X_test_input_ids, X_test_fasttext])\ny_pred_bert = np.argmax(y_prob_bert, axis=1)\n\nbert_roc_auc_score = roc_auc_score(to_categorical(y_test, num_classes=3), y_prob_bert, multi_class='ovr')\nbert_accuracy_score = accuracy_score(y_test, y_pred_bert)\n\nprint('Model overall ROC AUC score: {:.3f}'.format(bert_roc_auc_score))\nprint('Model overall accuracy: {:.3f}'.format(bert_accuracy_score))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Combine train data\nX_train_encoded = {key: tf.concat([X_train1_encoded[key], X_train2_encoded[key], X_train3_encoded[key]], axis=0) for key in X_train1_encoded.keys()}\nX_train_embeddings = np.concatenate([X_train1_embeddings, X_train2_embeddings, X_train3_embeddings], axis=0)\ny_train = np.concatenate([y_train1, y_train2, y_train3], axis=0)\nweights = np.concatenate([np.full(len(X_train1), 0.3), np.full(len(X_train2), 0.3), np.full(len(X_train3), 0.4)])\n\n# Combine test data\nX_test_encoded = {key: tf.concat([X_test1_encoded[key], X_test2_encoded[key], X_test3_encoded[key]], axis=0) for key in X_test1_encoded.keys()}\nX_test_embeddings = np.concatenate([X_test1_embeddings, X_test2_embeddings, X_test3_embeddings], axis=0)\ny_test = np.concatenate([y_test1, y_test2, y_test3], axis=0)\n\n# Define model architecture\ndef create_model():\n    bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n    input_ids = Input(shape=(max_length,), dtype=tf.int32, name='input_ids')\n    print(type(input_ids))\n    attention_mask = Input(shape=(max_length,), dtype=tf.int32, name='attention_mask')\n    fasttext_embeddings = Input(shape=(100,), dtype=tf.float32, name='fasttext_embeddings')\n    \n    # BERT embeddings\n    bert_output = bert_model(input_ids)[0]\n    dropout = Dropout(0.1)(bert_output[:, 0, :])  # Use the output of [CLS] token\n\n    # Concatenate BERT embeddings with FastText embeddings\n    concatenated_features = Concatenate()([dropout, fasttext_embeddings])\n\n    # Define model architecture\n    dense = Dense(128, activation='relu')(concatenated_features)\n    dropout = Dropout(0.1)(dense)\n    output = Dense(3, activation='softmax')(dropout)\n    \n    model = Model(inputs=[input_ids, attention_mask, fasttext_embeddings], outputs=output)\n    return model\n\nmodel = create_model()\n\n# Define early stopping callback\nearly_stopping = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n\n# Compile model\noptimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\nmodel.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train model\nhistory = model.fit(\n    [X_train_encoded['input_ids'], X_train_encoded['attention_mask'], X_train_embeddings],\n    tf.keras.utils.to_categorical(y_train, num_classes=3),\n    sample_weight=weights,\n    epochs=7,\n    batch_size=32,\n    validation_split=0.2,\n    callbacks=[early_stopping]\n)\n\n# Evaluate model\nloss, accuracy = model.evaluate(\n    [X_test_encoded['input_ids'], X_test_encoded['attention_mask'], X_test_embeddings],\n    tf.keras.utils.to_categorical(y_test, num_classes=3)\n)\nprint(f'Test Loss: {loss}, Test Accuracy: {accuracy}')\n\ny_prob_bert = model.predict([X_test_encoded['input_ids'], X_test_encoded['attention_mask'], X_test_embeddings])\ny_pred_bert = np.argmax(y_prob_bert, axis=1)\n\nbert_roc_auc_score = roc_auc_score(tf.keras.utils.to_categorical(y_test, num_classes=3), y_prob_bert, multi_class='ovr')\nbert_accuracy_score = accuracy_score(y_test, y_pred_bert)\n\nprint('Model overall ROC AUC score: {:.3f}'.format(bert_roc_auc_score))\nprint('Model overall accuracy: {:.3f}'.format(bert_accuracy_score))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n# model = BertModel.from_pretrained('bert-base-uncased')\n\n# def extract_features(text):\n#     inputs = tokenizer(text, return_tensors='pt', max_length=512, truncation=True, padding=True)\n#     outputs = model(**inputs)\n#     return outputs.last_hidden_state.mean(dim=1).detach().numpy()\n\n# for df in [disney, universal, museum]:\n#     df['features'] = df['cleaned_review'].apply(lambda x: extract_features(x).flatten())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X_train1, X_val1, y_train1, y_val1 = train_test_split(\n#     np.stack(disney['features'].values), disney['sentiment'].values, test_size=0.2, random_state=42)\n\n# X_train2, X_val2, y_train2, y_val2 = train_test_split(\n#     np.stack(universal['features'].values), universal['sentiment'].values, test_size=0.2, random_state=42)\n\n# X_train3, X_val3, y_train3, y_val3 = train_test_split(\n#     np.stack(museum['features'].values), museum['sentiment'].values, test_size=0.2, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Combine the training data\n# X_train = np.concatenate((X_train1, X_train2, X_train3), axis=0)\n# y_train = np.concatenate((y_train1, y_train2, y_train3), axis=0)\n\n# # Assign weights to the datasets\n# weights = np.concatenate((np.full(len(X_train1), 0.3), np.full(len(X_train2), 0.3), np.full(len(X_train3), 0.4)))\n\n# # Train the logistic regression model\n# model = LogisticRegression(max_iter=1000)\n# model.fit(X_train, y_train, sample_weight=weights)\n\n# # Combine the validation data\n# X_val = np.concatenate((X_val1, X_val2, X_val3), axis=0)\n# y_val = np.concatenate((y_val1, y_val2, y_val3), axis=0)\n\n# # Evaluate the model\n# y_pred = model.predict(X_val)\n# accuracy = accuracy_score(y_val, y_pred)\n# print(f'Validation Accuracy: {accuracy}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Print accuracy\n# print(f'Validation Accuracy: {accuracy}')\n# print(classification_report(y_val, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}