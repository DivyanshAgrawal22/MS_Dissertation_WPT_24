{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1858576,"sourceType":"datasetVersion","datasetId":1105807},{"sourceId":2304311,"sourceType":"datasetVersion","datasetId":1389694},{"sourceId":4547702,"sourceType":"datasetVersion","datasetId":2655259},{"sourceId":8958593,"sourceType":"datasetVersion","datasetId":5391899}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2024-07-15T21:55:35.316951Z","iopub.execute_input":"2024-07-15T21:55:35.317230Z","iopub.status.idle":"2024-07-15T21:55:47.111046Z","shell.execute_reply.started":"2024-07-15T21:55:35.317204Z","shell.execute_reply":"2024-07-15T21:55:47.109776Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.42.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.23.4)\nRequirement already satisfied: numpy<2.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.5.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import train_test_split\nfrom gensim.models import FastText\nfrom transformers import BertTokenizer, TFBertModel, TFBertForSequenceClassification\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Dropout, Concatenate, Embedding\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","metadata":{"execution":{"iopub.status.busy":"2024-07-15T22:32:19.138441Z","iopub.execute_input":"2024-07-15T22:32:19.138920Z","iopub.status.idle":"2024-07-15T22:32:19.146055Z","shell.execute_reply.started":"2024-07-15T22:32:19.138884Z","shell.execute_reply":"2024-07-15T22:32:19.144934Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"disney = pd.read_csv('/kaggle/input/disneyland-reviews/DisneylandReviews.csv', encoding='latin-1')\nuniversal = pd.read_csv('/kaggle/input/reviewuniversalstudio/universal_studio_branches.csv')\nmuseum = pd.read_csv('/kaggle/input/trip-advisor-review-british-museum-in-london/Data_Review_British_Museum.csv')\nnewquay = pd.read_csv('/kaggle/input/wild-planet-trust-review-data/Newquay Reviews.csv')\npaignton = pd.read_csv('/kaggle/input/wild-planet-trust-review-data/Paignton Reviews.csv')","metadata":{"execution":{"iopub.status.busy":"2024-07-15T21:56:01.781033Z","iopub.execute_input":"2024-07-15T21:56:01.781614Z","iopub.status.idle":"2024-07-15T21:56:02.886976Z","shell.execute_reply.started":"2024-07-15T21:56:01.781582Z","shell.execute_reply":"2024-07-15T21:56:02.885920Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"disney.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-15T21:56:02.889721Z","iopub.execute_input":"2024-07-15T21:56:02.890064Z","iopub.status.idle":"2024-07-15T21:56:02.907338Z","shell.execute_reply.started":"2024-07-15T21:56:02.890033Z","shell.execute_reply":"2024-07-15T21:56:02.906504Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"   Review_ID  Rating Year_Month     Reviewer_Location  \\\n0  670772142       4     2019-4             Australia   \n1  670682799       4     2019-5           Philippines   \n2  670623270       4     2019-4  United Arab Emirates   \n3  670607911       4     2019-4             Australia   \n4  670607296       4     2019-4        United Kingdom   \n\n                                         Review_Text               Branch  \n0  If you've ever been to Disneyland anywhere you...  Disneyland_HongKong  \n1  Its been a while since d last time we visit HK...  Disneyland_HongKong  \n2  Thanks God it wasn   t too hot or too humid wh...  Disneyland_HongKong  \n3  HK Disneyland is a great compact park. Unfortu...  Disneyland_HongKong  \n4  the location is not in the city, took around 1...  Disneyland_HongKong  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Review_ID</th>\n      <th>Rating</th>\n      <th>Year_Month</th>\n      <th>Reviewer_Location</th>\n      <th>Review_Text</th>\n      <th>Branch</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>670772142</td>\n      <td>4</td>\n      <td>2019-4</td>\n      <td>Australia</td>\n      <td>If you've ever been to Disneyland anywhere you...</td>\n      <td>Disneyland_HongKong</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>670682799</td>\n      <td>4</td>\n      <td>2019-5</td>\n      <td>Philippines</td>\n      <td>Its been a while since d last time we visit HK...</td>\n      <td>Disneyland_HongKong</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>670623270</td>\n      <td>4</td>\n      <td>2019-4</td>\n      <td>United Arab Emirates</td>\n      <td>Thanks God it wasn   t too hot or too humid wh...</td>\n      <td>Disneyland_HongKong</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>670607911</td>\n      <td>4</td>\n      <td>2019-4</td>\n      <td>Australia</td>\n      <td>HK Disneyland is a great compact park. Unfortu...</td>\n      <td>Disneyland_HongKong</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>670607296</td>\n      <td>4</td>\n      <td>2019-4</td>\n      <td>United Kingdom</td>\n      <td>the location is not in the city, took around 1...</td>\n      <td>Disneyland_HongKong</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"disney.drop(columns=['Review_ID', 'Branch', 'Reviewer_Location'], inplace=True)\ndisney.rename(columns={'Year_Month': 'written_date', 'Review_Text': 'review', 'Rating': 'rating'}, inplace=True)\ndisney.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-15T21:56:02.908461Z","iopub.execute_input":"2024-07-15T21:56:02.908785Z","iopub.status.idle":"2024-07-15T21:56:02.923363Z","shell.execute_reply.started":"2024-07-15T21:56:02.908758Z","shell.execute_reply":"2024-07-15T21:56:02.922385Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"   rating written_date                                             review\n0       4       2019-4  If you've ever been to Disneyland anywhere you...\n1       4       2019-5  Its been a while since d last time we visit HK...\n2       4       2019-4  Thanks God it wasn   t too hot or too humid wh...\n3       4       2019-4  HK Disneyland is a great compact park. Unfortu...\n4       4       2019-4  the location is not in the city, took around 1...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>rating</th>\n      <th>written_date</th>\n      <th>review</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4</td>\n      <td>2019-4</td>\n      <td>If you've ever been to Disneyland anywhere you...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>2019-5</td>\n      <td>Its been a while since d last time we visit HK...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>2019-4</td>\n      <td>Thanks God it wasn   t too hot or too humid wh...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>2019-4</td>\n      <td>HK Disneyland is a great compact park. Unfortu...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>2019-4</td>\n      <td>the location is not in the city, took around 1...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"universal.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-15T21:56:02.924611Z","iopub.execute_input":"2024-07-15T21:56:02.925390Z","iopub.status.idle":"2024-07-15T21:56:02.938117Z","shell.execute_reply.started":"2024-07-15T21:56:02.925351Z","shell.execute_reply":"2024-07-15T21:56:02.936999Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"          reviewer  rating  written_date  \\\n0          Kelly B     2.0  May 30, 2021   \n1              Jon     1.0  May 30, 2021   \n2          Nerdy P     2.0  May 30, 2021   \n3        ran101278     4.0  May 29, 2021   \n4  tammies20132015     5.0  May 28, 2021   \n\n                                               title  \\\n0  Universal is a complete Disaster - stick with ...   \n1                               Food is hard to get.   \n2                                       Disappointed   \n3                                         My opinion   \n4                  The Bourne Stuntacular...MUST SEE   \n\n                                         review_text  \\\n0  We went to Universal over Memorial Day weekend...   \n1  The food service is horrible. I’m not reviewin...   \n2  I booked this vacation mainly to ride Hagrid m...   \n3  When a person tries the test seat for the ride...   \n4  Ok, I can't stress enough to anyone and everyo...   \n\n                      branch  \n0  Universal Studios Florida  \n1  Universal Studios Florida  \n2  Universal Studios Florida  \n3  Universal Studios Florida  \n4  Universal Studios Florida  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>reviewer</th>\n      <th>rating</th>\n      <th>written_date</th>\n      <th>title</th>\n      <th>review_text</th>\n      <th>branch</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Kelly B</td>\n      <td>2.0</td>\n      <td>May 30, 2021</td>\n      <td>Universal is a complete Disaster - stick with ...</td>\n      <td>We went to Universal over Memorial Day weekend...</td>\n      <td>Universal Studios Florida</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Jon</td>\n      <td>1.0</td>\n      <td>May 30, 2021</td>\n      <td>Food is hard to get.</td>\n      <td>The food service is horrible. I’m not reviewin...</td>\n      <td>Universal Studios Florida</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Nerdy P</td>\n      <td>2.0</td>\n      <td>May 30, 2021</td>\n      <td>Disappointed</td>\n      <td>I booked this vacation mainly to ride Hagrid m...</td>\n      <td>Universal Studios Florida</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ran101278</td>\n      <td>4.0</td>\n      <td>May 29, 2021</td>\n      <td>My opinion</td>\n      <td>When a person tries the test seat for the ride...</td>\n      <td>Universal Studios Florida</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>tammies20132015</td>\n      <td>5.0</td>\n      <td>May 28, 2021</td>\n      <td>The Bourne Stuntacular...MUST SEE</td>\n      <td>Ok, I can't stress enough to anyone and everyo...</td>\n      <td>Universal Studios Florida</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"universal['review'] = universal.title + \" \" + universal.review_text\nuniversal.drop(columns=['reviewer', 'branch', 'title', 'review_text'], inplace=True)\nuniversal.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-15T21:56:02.939807Z","iopub.execute_input":"2024-07-15T21:56:02.940517Z","iopub.status.idle":"2024-07-15T21:56:03.021483Z","shell.execute_reply.started":"2024-07-15T21:56:02.940479Z","shell.execute_reply":"2024-07-15T21:56:03.020213Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"   rating  written_date                                             review\n0     2.0  May 30, 2021  Universal is a complete Disaster - stick with ...\n1     1.0  May 30, 2021  Food is hard to get. The food service is horri...\n2     2.0  May 30, 2021  Disappointed I booked this vacation mainly to ...\n3     4.0  May 29, 2021  My opinion When a person tries the test seat f...\n4     5.0  May 28, 2021  The Bourne Stuntacular...MUST SEE Ok, I can't ...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>rating</th>\n      <th>written_date</th>\n      <th>review</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2.0</td>\n      <td>May 30, 2021</td>\n      <td>Universal is a complete Disaster - stick with ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.0</td>\n      <td>May 30, 2021</td>\n      <td>Food is hard to get. The food service is horri...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2.0</td>\n      <td>May 30, 2021</td>\n      <td>Disappointed I booked this vacation mainly to ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4.0</td>\n      <td>May 29, 2021</td>\n      <td>My opinion When a person tries the test seat f...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.0</td>\n      <td>May 28, 2021</td>\n      <td>The Bourne Stuntacular...MUST SEE Ok, I can't ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"museum.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-15T21:56:03.022963Z","iopub.execute_input":"2024-07-15T21:56:03.025916Z","iopub.status.idle":"2024-07-15T21:56:03.038878Z","shell.execute_reply.started":"2024-07-15T21:56:03.025872Z","shell.execute_reply":"2024-07-15T21:56:03.037865Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                                     title  \\\n0         Remarkable Repository of Culture   \n1                           Amazing Museum   \n2              Beautiful, must see museum!   \n3                    Make sure you donate!   \n4  Epic Museum with something for everyone   \n\n                                             comment           trip  \\\n0  I was overwhelmed with this cornucopia of hist...   October 2020   \n1  British Museum always worth a visit no matter ...  February 2020   \n2  I went before we had tier 4 in London and I fe...  November 2020   \n3  I only have good things to say about The Briti...  November 2020   \n4  From the walk up to the building and then ente...   January 2020   \n\n            writer   written  rating  \n0       paulyMaine    Jan 10       5  \n1         Tati_Luz     Jan 5       5  \n2  raynerjosephine  Dec 2020       5  \n3             Ipek  Dec 2020       5  \n4        Anthony G  Dec 2020       5  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>comment</th>\n      <th>trip</th>\n      <th>writer</th>\n      <th>written</th>\n      <th>rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Remarkable Repository of Culture</td>\n      <td>I was overwhelmed with this cornucopia of hist...</td>\n      <td>October 2020</td>\n      <td>paulyMaine</td>\n      <td>Jan 10</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Amazing Museum</td>\n      <td>British Museum always worth a visit no matter ...</td>\n      <td>February 2020</td>\n      <td>Tati_Luz</td>\n      <td>Jan 5</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Beautiful, must see museum!</td>\n      <td>I went before we had tier 4 in London and I fe...</td>\n      <td>November 2020</td>\n      <td>raynerjosephine</td>\n      <td>Dec 2020</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Make sure you donate!</td>\n      <td>I only have good things to say about The Briti...</td>\n      <td>November 2020</td>\n      <td>Ipek</td>\n      <td>Dec 2020</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Epic Museum with something for everyone</td>\n      <td>From the walk up to the building and then ente...</td>\n      <td>January 2020</td>\n      <td>Anthony G</td>\n      <td>Dec 2020</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"museum['review'] = museum.title + \" \" + museum.comment\nmuseum.drop(columns=['title', 'comment', 'trip', 'writer'], inplace=True)\nmuseum.rename(columns={'written': 'written_date'}, inplace=True)\nmuseum.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-15T21:56:03.040204Z","iopub.execute_input":"2024-07-15T21:56:03.040558Z","iopub.status.idle":"2024-07-15T21:56:03.057143Z","shell.execute_reply.started":"2024-07-15T21:56:03.040531Z","shell.execute_reply":"2024-07-15T21:56:03.056087Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"  written_date  rating                                             review\n0       Jan 10       5  Remarkable Repository of Culture I was overwhe...\n1        Jan 5       5  Amazing Museum British Museum always worth a v...\n2     Dec 2020       5  Beautiful, must see museum! I went before we h...\n3     Dec 2020       5  Make sure you donate! I only have good things ...\n4     Dec 2020       5  Epic Museum with something for everyone From t...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>written_date</th>\n      <th>rating</th>\n      <th>review</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Jan 10</td>\n      <td>5</td>\n      <td>Remarkable Repository of Culture I was overwhe...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Jan 5</td>\n      <td>5</td>\n      <td>Amazing Museum British Museum always worth a v...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Dec 2020</td>\n      <td>5</td>\n      <td>Beautiful, must see museum! I went before we h...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Dec 2020</td>\n      <td>5</td>\n      <td>Make sure you donate! I only have good things ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Dec 2020</td>\n      <td>5</td>\n      <td>Epic Museum with something for everyone From t...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Download stopwords\nnltk.download('stopwords')\nstop_words = set(stopwords.words('english'))\n\ndef preprocess_text(text):\n    text = text.lower()\n    text = re.sub(r'\\d+', '', text)\n    text = re.sub(r'[^\\w\\s]', '', text)\n    text = re.sub(r'\\s+', ' ', text).strip()\n    text = ' '.join([word for word in text.split() if word not in stop_words])\n    return text\n\nfor df in [disney, universal, museum]:\n    df['cleaned_review'] = df['review'].apply(preprocess_text)\n    df['sentiment'] = df['rating'].apply(lambda x: 1 if x >= 4 else 0)\n    \n# Split datasets into train and test sets\ndef split_data(df):\n    X_train, X_test, y_train, y_test = train_test_split(df['cleaned_review'], df['sentiment'], test_size=0.2, random_state=42)\n    return X_train, X_test, y_train, y_test\n\nX_train1, X_test1, y_train1, y_test1 = split_data(disney)\nX_train2, X_test2, y_train2, y_test2 = split_data(universal)\nX_train3, X_test3, y_train3, y_test3 = split_data(museum)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T21:56:03.058520Z","iopub.execute_input":"2024-07-15T21:56:03.058874Z","iopub.status.idle":"2024-07-15T21:56:12.768317Z","shell.execute_reply.started":"2024-07-15T21:56:03.058847Z","shell.execute_reply":"2024-07-15T21:56:12.767250Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"# Train FastText model\nfasttext_model = FastText(sentences=pd.concat([X_train1, X_train2, X_train3]).tolist(), window=5, min_count=5, workers=4)\n\n# Function to generate FastText embeddings\ndef generate_fasttext_embeddings(text_data):\n    embeddings = []\n    for text in text_data:\n        embedding = [fasttext_model.wv[word] for word in text.split() if word in fasttext_model.wv]\n        if embedding:\n            embeddings.append(np.mean(embedding, axis=0))\n        else:\n            embeddings.append(np.zeros(100))  # Handle out-of-vocabulary words\n    return np.array(embeddings)\n\n# Generate FastText embeddings for train and test data\nX_train1_embeddings = generate_fasttext_embeddings(X_train1)\nX_test1_embeddings = generate_fasttext_embeddings(X_test1)\n\nX_train2_embeddings = generate_fasttext_embeddings(X_train2)\nX_test2_embeddings = generate_fasttext_embeddings(X_test2)\n\nX_train3_embeddings = generate_fasttext_embeddings(X_train3)\nX_test3_embeddings = generate_fasttext_embeddings(X_test3)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T21:56:12.769643Z","iopub.execute_input":"2024-07-15T21:56:12.769980Z","iopub.status.idle":"2024-07-15T21:59:32.269963Z","shell.execute_reply.started":"2024-07-15T21:56:12.769953Z","shell.execute_reply":"2024-07-15T21:59:32.268887Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmax_length = 128\n\n# Function to tokenize and encode text data\ndef tokenize_and_encode(text_data):\n    return tokenizer(text_data.tolist(), padding='max_length', truncation=True, max_length=max_length, return_tensors='tf')\n\n# Tokenize and encode train and test data\nX_train1_encoded = tokenize_and_encode(X_train1)\nX_test1_encoded = tokenize_and_encode(X_test1)\n\nX_train2_encoded = tokenize_and_encode(X_train2)\nX_test2_encoded = tokenize_and_encode(X_test2)\n\nX_train3_encoded = tokenize_and_encode(X_train3)\nX_test3_encoded = tokenize_and_encode(X_test3)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T21:59:32.271816Z","iopub.execute_input":"2024-07-15T21:59:32.272882Z","iopub.status.idle":"2024-07-15T22:02:41.443457Z","shell.execute_reply.started":"2024-07-15T21:59:32.272840Z","shell.execute_reply":"2024-07-15T22:02:41.442591Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"X_train1_encoded","metadata":{"execution":{"iopub.status.busy":"2024-07-15T22:02:41.446252Z","iopub.execute_input":"2024-07-15T22:02:41.446578Z","iopub.status.idle":"2024-07-15T22:02:41.454387Z","shell.execute_reply.started":"2024-07-15T22:02:41.446552Z","shell.execute_reply":"2024-07-15T22:02:41.453408Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"{'input_ids': <tf.Tensor: shape=(34124, 128), dtype=int32, numpy=\narray([[  101,  2307,  2173, ...,     0,     0,     0],\n       [  101,  4067,  6919, ...,     0,     0,     0],\n       [  101,  2052, 16755, ...,     0,     0,     0],\n       ...,\n       [  101,  2190,  4536, ...,     0,     0,     0],\n       [  101,  3517,  6373, ...,     0,     0,     0],\n       [  101, 25104,  2175, ...,  2662,  6172,   102]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(34124, 128), dtype=int32, numpy=\narray([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(34124, 128), dtype=int32, numpy=\narray([[1, 1, 1, ..., 0, 0, 0],\n       [1, 1, 1, ..., 0, 0, 0],\n       [1, 1, 1, ..., 0, 0, 0],\n       ...,\n       [1, 1, 1, ..., 0, 0, 0],\n       [1, 1, 1, ..., 0, 0, 0],\n       [1, 1, 1, ..., 1, 1, 1]], dtype=int32)>}"},"metadata":{}}]},{"cell_type":"code","source":"# Assuming you have precomputed FastText embeddings\nembedding_dim = 300  # Adjust this according to your FastText embedding dimensions\n\n# Define inputs\ninput_ids = Input(shape=(128,), dtype=tf.int32, name='input_ids')\ntoken_type_ids = Input(shape=(128,), dtype=tf.int32, name='token_type_ids')\nattention_mask = Input(shape=(128,), dtype=tf.int32, name='attention_mask')\n\n# Embedding layer (assuming precomputed FastText embeddings)\nembedding_layer = Embedding(input_dim=100, output_dim=embedding_dim, trainable=True)\nembeddings = embedding_layer(input_ids)\n\n# Concatenate embeddings with token_type_ids and attention_mask\nx = Concatenate()([embeddings, tf.expand_dims(tf.cast(token_type_ids, tf.float32), axis=-1), tf.expand_dims(tf.cast(attention_mask, tf.float32), axis=-1)])\n\n# Flatten or use GlobalAveragePooling1D to prepare for dense layers\nx = Flatten()(x)\n\n# ANN layers\nx = Dense(256, activation='relu')(x)\nx = Dense(128, activation='relu')(x)\nx = Dense(64, activation='relu')(x)\n\n# Output layer for binary classification with sigmoid activation\noutputs = Dense(1, activation='sigmoid')(x)\n\n# Define the model\nmodel = Model(inputs=[input_ids, token_type_ids, attention_mask], outputs=outputs)\n\n# Compile the model\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\n# Assuming you have three weighted datasets and corresponding test sets\ntrain_weights = [0.3, 0.3, 0.4]\n\n# Assuming train_1, train_2, train_3 are your weighted training datasets\n# train_1, train_2, train_3 should be tuples of (input_ids, token_type_ids, attention_mask, labels)\n\n# Concatenate all input_ids, token_type_ids, attention_mask, and labels\nall_input_ids = tf.concat([X_train1_encoded[0], X_train2_encoded[0], X_train3_encoded[0]], axis=0)\nall_token_type_ids = tf.concat([X_train1_encoded[1], X_train2_encoded[1], X_train3_encoded[1]], axis=0)\nall_attention_mask = tf.concat([X_train1_encoded[2], X_train2_encoded[2], X_train3_encoded[2]], axis=0)\nall_labels = tf.concat([X_train1_encoded[3], X_train2_encoded[3], X_train3_encoded[3]], axis=0)\n\n# Train the model on all datasets with respective weights\nhistory = model.fit([all_input_ids, all_token_type_ids, all_attention_mask], all_labels,\n                    epochs=10,\n                    batch_size=32,\n                    sample_weight=[train_weights[0] * tf.ones_like(X_train1_encoded[0][:, 0]),\n                                   train_weights[1] * tf.ones_like(X_train2_encoded[0][:, 0]),\n                                   train_weights[2] * tf.ones_like(X_train3_encoded[0][:, 0])])\n\n# Evaluate on test datasets\ntest_results_1 = model.evaluate([X_test1_encoding[0], X_test1_encoding[1], X_test1_encoding[2]], X_test1_encoding[3])\ntest_results_2 = model.evaluate([X_test2_encoding[0], X_test2_encoding[1], X_test2_encoding[2]], X_test2_encoding[3])\ntest_results_3 = model.evaluate([X_test3_encoding[0], X_test3_encoding[1], X_test3_encoding[2]], X_test3_encoding[3])\n\nprint(\"Test results for test_1:\")\nprint(f\"Loss: {test_results_1[0]}, Accuracy: {test_results_1[1]}\")\n\nprint(\"\\nTest results for test_2:\")\nprint(f\"Loss: {test_results_2[0]}, Accuracy: {test_results_2[1]}\")\n\nprint(\"\\nTest results for test_3:\")\nprint(f\"Loss: {test_results_3[0]}, Accuracy: {test_results_3[1]}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-15T22:32:23.438419Z","iopub.execute_input":"2024-07-15T22:32:23.439319Z","iopub.status.idle":"2024-07-15T22:32:23.573247Z","shell.execute_reply.started":"2024-07-15T22:32:23.439278Z","shell.execute_reply":"2024-07-15T22:32:23.571755Z"},"trusted":true},"execution_count":23,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[23], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m embedding_layer(input_ids)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Concatenate embeddings with token_type_ids and attention_mask\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m x \u001b[38;5;241m=\u001b[39m Concatenate()([embeddings, tf\u001b[38;5;241m.\u001b[39mexpand_dims(\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), tf\u001b[38;5;241m.\u001b[39mexpand_dims(tf\u001b[38;5;241m.\u001b[39mcast(attention_mask, tf\u001b[38;5;241m.\u001b[39mfloat32), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)])\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Flatten or use GlobalAveragePooling1D to prepare for dense layers\u001b[39;00m\n\u001b[1;32m     17\u001b[0m x \u001b[38;5;241m=\u001b[39m Flatten()(x)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/backend/common/keras_tensor.py:91\u001b[0m, in \u001b[0;36mKerasTensor.__tf_tensor__\u001b[0;34m(self, dtype, name)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__tf_tensor__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 91\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     92\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA KerasTensor cannot be used as input to a TensorFlow function. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     93\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA KerasTensor is a symbolic placeholder for a shape and dtype, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mused when constructing Keras Functional models \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     95\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor Keras Functions. You can only use it as input to a Keras layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     96\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor a Keras operation (from the namespaces `keras.layers` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand `keras.operations`). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are likely doing something like:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx = Input(...)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf_fn(x)  # Invalid.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat you should do instead is wrap `tf_fn` in a layer:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    105\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    106\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass MyLayer(Layer):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    107\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    def call(self, x):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        return tf_fn(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    109\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx = MyLayer()(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    110\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    111\u001b[0m     )\n","\u001b[0;31mValueError\u001b[0m: A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n"],"ename":"ValueError","evalue":"A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n","output_type":"error"}]},{"cell_type":"code","source":"# Function to create the model\ndef create_model():\n    # Define the BERT model\n    bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n\n    # Define input layers\n    input_ids = Input(shape=(100,), dtype=tf.int32, name='input_ids')\n#     attention_mask = Input(shape=(128,), dtype=tf.int32, name='attention_mask')\n    fasttext_embeddings = Input(shape=(100,), dtype=tf.float32, name='fasttext_embeddings')\n\n    # BERT embeddings\n    bert_output = bert_model(input_ids)[0]\n    \n    # Dropout some samples randomly\n    dropout = Dropout(0.1)(bert_output)\n\n    # Concatenate BERT embeddings with FastText embeddings\n    concatenated_features = Concatenate()([dropout, fasttext_embeddings])\n\n    # Define model architecture\n    dense = Dense(128, activation='relu')(concatenated_features)\n    dropout = Dropout(0.1)(dense)\n    output = Dense(3, activation='softmax')(dropout)\n    \n    model = Model(inputs=[input_ids, fasttext_embeddings], outputs=output)\n    return model\n\n# Define model\nmodel = create_model()\n\n# Define early stopping callback\nearly_stopping = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n\n# Compile model\noptimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\nmodel.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Training dataset weights\ntrain_weights = np.concatenate([\n    np.full(len(train_1), 0.3),\n    np.full(len(train_2), 0.3),\n    np.full(len(train_3), 0.4)\n])\n\n# Combine the training datasets\nX_train_input_ids = np.concatenate([train_1_input_ids, train_2_input_ids, train_3_input_ids])\nX_train_fasttext = np.concatenate([train_1_fasttext, train_2_fasttext, train_3_fasttext])\ny_train = np.concatenate([train_1_labels, train_2_labels, train_3_labels])\n\n# Train model\nhistory = model.fit(\n    [X_train_input_ids, X_train_fasttext], \n    to_categorical(y_train, num_classes=3), \n    sample_weight=train_weights,\n    epochs=7, \n    batch_size=32, \n    validation_split=0.2, \n    callbacks=[early_stopping]\n)\n\n# Combine the test datasets\nX_test_input_ids = np.concatenate([test_1_input_ids, test_2_input_ids, test_3_input_ids])\nX_test_fasttext = np.concatenate([test_1_fasttext, test_2_fasttext, test_3_fasttext])\ny_test = np.concatenate([test_1_labels, test_2_labels, test_3_labels])\n\n# Evaluate model\nloss, accuracy = model.evaluate(\n    [X_test_input_ids, X_test_fasttext], \n    to_categorical(y_test, num_classes=3)\n)\nprint(f'Test Loss: {loss}, Test Accuracy: {accuracy}')\n\n# Predict and evaluate performance\ny_prob_bert = model.predict([X_test_input_ids, X_test_fasttext])\ny_pred_bert = np.argmax(y_prob_bert, axis=1)\n\nbert_roc_auc_score = roc_auc_score(to_categorical(y_test, num_classes=3), y_prob_bert, multi_class='ovr')\nbert_accuracy_score = accuracy_score(y_test, y_pred_bert)\n\nprint('Model overall ROC AUC score: {:.3f}'.format(bert_roc_auc_score))\nprint('Model overall accuracy: {:.3f}'.format(bert_accuracy_score))","metadata":{"execution":{"iopub.status.busy":"2024-07-15T22:16:08.477796Z","iopub.execute_input":"2024-07-15T22:16:08.478201Z","iopub.status.idle":"2024-07-15T22:16:11.535435Z","shell.execute_reply.started":"2024-07-15T22:16:08.478172Z","shell.execute_reply":"2024-07-15T22:16:11.533854Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n\nSome weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[20], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Define model\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Define early stopping callback\u001b[39;00m\n\u001b[1;32m     32\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","Cell \u001b[0;32mIn[20], line 12\u001b[0m, in \u001b[0;36mcreate_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m fasttext_embeddings \u001b[38;5;241m=\u001b[39m Input(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m100\u001b[39m,), dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfasttext_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# BERT embeddings\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m bert_output \u001b[38;5;241m=\u001b[39m \u001b[43mbert_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Dropout some samples randomly\u001b[39;00m\n\u001b[1;32m     15\u001b[0m dropout \u001b[38;5;241m=\u001b[39m Dropout(\u001b[38;5;241m0.1\u001b[39m)(bert_output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_tf_utils.py:436\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    434\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m--> 436\u001b[0m unpacked_inputs \u001b[38;5;241m=\u001b[39m \u001b[43minput_processing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args_and_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munpacked_inputs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_tf_utils.py:566\u001b[0m, in \u001b[0;36minput_processing\u001b[0;34m(func, config, **kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m         output[main_input_name] \u001b[38;5;241m=\u001b[39m main_input\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 566\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    567\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(main_input)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not allowed only \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mallowed_types\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is accepted for\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmain_input_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m         )\n\u001b[1;32m    571\u001b[0m \u001b[38;5;66;03m# Populates any unspecified argument with their default value, according to the signature.\u001b[39;00m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m parameter_names:\n","\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer 'tf_bert_for_sequence_classification_5' (type TFBertForSequenceClassification).\n\nData of type <class 'keras.src.backend.common.keras_tensor.KerasTensor'> is not allowed only (<class 'tensorflow.python.framework.tensor.Tensor'>, <class 'bool'>, <class 'int'>, <class 'transformers.utils.generic.ModelOutput'>, <class 'tuple'>, <class 'list'>, <class 'dict'>, <class 'numpy.ndarray'>) is accepted for input_ids.\n\nCall arguments received by layer 'tf_bert_for_sequence_classification_5' (type TFBertForSequenceClassification):\n  • input_ids=<KerasTensor shape=(None, 100), dtype=int32, sparse=None, name=input_ids>\n  • attention_mask=None\n  • token_type_ids=None\n  • position_ids=None\n  • head_mask=None\n  • inputs_embeds=None\n  • output_attentions=None\n  • output_hidden_states=None\n  • return_dict=None\n  • labels=None\n  • training=False"],"ename":"ValueError","evalue":"Exception encountered when calling layer 'tf_bert_for_sequence_classification_5' (type TFBertForSequenceClassification).\n\nData of type <class 'keras.src.backend.common.keras_tensor.KerasTensor'> is not allowed only (<class 'tensorflow.python.framework.tensor.Tensor'>, <class 'bool'>, <class 'int'>, <class 'transformers.utils.generic.ModelOutput'>, <class 'tuple'>, <class 'list'>, <class 'dict'>, <class 'numpy.ndarray'>) is accepted for input_ids.\n\nCall arguments received by layer 'tf_bert_for_sequence_classification_5' (type TFBertForSequenceClassification):\n  • input_ids=<KerasTensor shape=(None, 100), dtype=int32, sparse=None, name=input_ids>\n  • attention_mask=None\n  • token_type_ids=None\n  • position_ids=None\n  • head_mask=None\n  • inputs_embeds=None\n  • output_attentions=None\n  • output_hidden_states=None\n  • return_dict=None\n  • labels=None\n  • training=False","output_type":"error"}]},{"cell_type":"code","source":"# Combine train data\nX_train_encoded = {key: tf.concat([X_train1_encoded[key], X_train2_encoded[key], X_train3_encoded[key]], axis=0) for key in X_train1_encoded.keys()}\nX_train_embeddings = np.concatenate([X_train1_embeddings, X_train2_embeddings, X_train3_embeddings], axis=0)\ny_train = np.concatenate([y_train1, y_train2, y_train3], axis=0)\nweights = np.concatenate([np.full(len(X_train1), 0.3), np.full(len(X_train2), 0.3), np.full(len(X_train3), 0.4)])\n\n# Combine test data\nX_test_encoded = {key: tf.concat([X_test1_encoded[key], X_test2_encoded[key], X_test3_encoded[key]], axis=0) for key in X_test1_encoded.keys()}\nX_test_embeddings = np.concatenate([X_test1_embeddings, X_test2_embeddings, X_test3_embeddings], axis=0)\ny_test = np.concatenate([y_test1, y_test2, y_test3], axis=0)\n\n# Define model architecture\ndef create_model():\n    bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n    input_ids = Input(shape=(max_length,), dtype=tf.int32, name='input_ids')\n    print(type(input_ids))\n    attention_mask = Input(shape=(max_length,), dtype=tf.int32, name='attention_mask')\n    fasttext_embeddings = Input(shape=(100,), dtype=tf.float32, name='fasttext_embeddings')\n    \n    # BERT embeddings\n    bert_output = bert_model(input_ids)[0]\n    dropout = Dropout(0.1)(bert_output[:, 0, :])  # Use the output of [CLS] token\n\n    # Concatenate BERT embeddings with FastText embeddings\n    concatenated_features = Concatenate()([dropout, fasttext_embeddings])\n\n    # Define model architecture\n    dense = Dense(128, activation='relu')(concatenated_features)\n    dropout = Dropout(0.1)(dense)\n    output = Dense(3, activation='softmax')(dropout)\n    \n    model = Model(inputs=[input_ids, attention_mask, fasttext_embeddings], outputs=output)\n    return model\n\nmodel = create_model()\n\n# Define early stopping callback\nearly_stopping = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n\n# Compile model\noptimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\nmodel.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train model\nhistory = model.fit(\n    [X_train_encoded['input_ids'], X_train_encoded['attention_mask'], X_train_embeddings],\n    tf.keras.utils.to_categorical(y_train, num_classes=3),\n    sample_weight=weights,\n    epochs=7,\n    batch_size=32,\n    validation_split=0.2,\n    callbacks=[early_stopping]\n)\n\n# Evaluate model\nloss, accuracy = model.evaluate(\n    [X_test_encoded['input_ids'], X_test_encoded['attention_mask'], X_test_embeddings],\n    tf.keras.utils.to_categorical(y_test, num_classes=3)\n)\nprint(f'Test Loss: {loss}, Test Accuracy: {accuracy}')\n\ny_prob_bert = model.predict([X_test_encoded['input_ids'], X_test_encoded['attention_mask'], X_test_embeddings])\ny_pred_bert = np.argmax(y_prob_bert, axis=1)\n\nbert_roc_auc_score = roc_auc_score(tf.keras.utils.to_categorical(y_test, num_classes=3), y_prob_bert, multi_class='ovr')\nbert_accuracy_score = accuracy_score(y_test, y_pred_bert)\n\nprint('Model overall ROC AUC score: {:.3f}'.format(bert_roc_auc_score))\nprint('Model overall accuracy: {:.3f}'.format(bert_accuracy_score))","metadata":{"execution":{"iopub.status.busy":"2024-07-15T22:02:41.912144Z","iopub.execute_input":"2024-07-15T22:02:41.912907Z","iopub.status.idle":"2024-07-15T22:02:45.616550Z","shell.execute_reply.started":"2024-07-15T22:02:41.912874Z","shell.execute_reply":"2024-07-15T22:02:45.615018Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of TFBertModel were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"<class 'keras.src.backend.common.keras_tensor.KerasTensor'>\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[14], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m     model \u001b[38;5;241m=\u001b[39m Model(inputs\u001b[38;5;241m=\u001b[39m[input_ids, attention_mask, fasttext_embeddings], outputs\u001b[38;5;241m=\u001b[39moutput)\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[0;32m---> 35\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Define early stopping callback\u001b[39;00m\n\u001b[1;32m     38\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","Cell \u001b[0;32mIn[14], line 21\u001b[0m, in \u001b[0;36mcreate_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m fasttext_embeddings \u001b[38;5;241m=\u001b[39m Input(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m100\u001b[39m,), dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfasttext_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# BERT embeddings\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m bert_output \u001b[38;5;241m=\u001b[39m \u001b[43mbert_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     22\u001b[0m dropout \u001b[38;5;241m=\u001b[39m Dropout(\u001b[38;5;241m0.1\u001b[39m)(bert_output[:, \u001b[38;5;241m0\u001b[39m, :])  \u001b[38;5;66;03m# Use the output of [CLS] token\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Concatenate BERT embeddings with FastText embeddings\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_tf_utils.py:436\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    434\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m--> 436\u001b[0m unpacked_inputs \u001b[38;5;241m=\u001b[39m \u001b[43minput_processing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args_and_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munpacked_inputs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_tf_utils.py:566\u001b[0m, in \u001b[0;36minput_processing\u001b[0;34m(func, config, **kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m         output[main_input_name] \u001b[38;5;241m=\u001b[39m main_input\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 566\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    567\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(main_input)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not allowed only \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mallowed_types\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is accepted for\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmain_input_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m         )\n\u001b[1;32m    571\u001b[0m \u001b[38;5;66;03m# Populates any unspecified argument with their default value, according to the signature.\u001b[39;00m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m parameter_names:\n","\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer 'tf_bert_model' (type TFBertModel).\n\nData of type <class 'keras.src.backend.common.keras_tensor.KerasTensor'> is not allowed only (<class 'tensorflow.python.framework.tensor.Tensor'>, <class 'bool'>, <class 'int'>, <class 'transformers.utils.generic.ModelOutput'>, <class 'tuple'>, <class 'list'>, <class 'dict'>, <class 'numpy.ndarray'>) is accepted for input_ids.\n\nCall arguments received by layer 'tf_bert_model' (type TFBertModel):\n  • input_ids=<KerasTensor shape=(None, 128), dtype=int32, sparse=None, name=input_ids>\n  • attention_mask=None\n  • token_type_ids=None\n  • position_ids=None\n  • head_mask=None\n  • inputs_embeds=None\n  • encoder_hidden_states=None\n  • encoder_attention_mask=None\n  • past_key_values=None\n  • use_cache=None\n  • output_attentions=None\n  • output_hidden_states=None\n  • return_dict=None\n  • training=False"],"ename":"ValueError","evalue":"Exception encountered when calling layer 'tf_bert_model' (type TFBertModel).\n\nData of type <class 'keras.src.backend.common.keras_tensor.KerasTensor'> is not allowed only (<class 'tensorflow.python.framework.tensor.Tensor'>, <class 'bool'>, <class 'int'>, <class 'transformers.utils.generic.ModelOutput'>, <class 'tuple'>, <class 'list'>, <class 'dict'>, <class 'numpy.ndarray'>) is accepted for input_ids.\n\nCall arguments received by layer 'tf_bert_model' (type TFBertModel):\n  • input_ids=<KerasTensor shape=(None, 128), dtype=int32, sparse=None, name=input_ids>\n  • attention_mask=None\n  • token_type_ids=None\n  • position_ids=None\n  • head_mask=None\n  • inputs_embeds=None\n  • encoder_hidden_states=None\n  • encoder_attention_mask=None\n  • past_key_values=None\n  • use_cache=None\n  • output_attentions=None\n  • output_hidden_states=None\n  • return_dict=None\n  • training=False","output_type":"error"}]},{"cell_type":"code","source":"# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n# model = BertModel.from_pretrained('bert-base-uncased')\n\n# def extract_features(text):\n#     inputs = tokenizer(text, return_tensors='pt', max_length=512, truncation=True, padding=True)\n#     outputs = model(**inputs)\n#     return outputs.last_hidden_state.mean(dim=1).detach().numpy()\n\n# for df in [disney, universal, museum]:\n#     df['features'] = df['cleaned_review'].apply(lambda x: extract_features(x).flatten())","metadata":{"execution":{"iopub.status.busy":"2024-07-15T13:58:20.681134Z","iopub.execute_input":"2024-07-15T13:58:20.681425Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"468d4589c65843a984107e497d43eed8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"501362093ec840259a9fdd3cb7051ae7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a43ea61a7594f5780a097f79def6b34"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce535e5b83064f168b09941ad12b29e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"691b2d8ec73b4c0fbb2ef0253b1276ba"}},"metadata":{}}]},{"cell_type":"code","source":"# X_train1, X_val1, y_train1, y_val1 = train_test_split(\n#     np.stack(disney['features'].values), disney['sentiment'].values, test_size=0.2, random_state=42)\n\n# X_train2, X_val2, y_train2, y_val2 = train_test_split(\n#     np.stack(universal['features'].values), universal['sentiment'].values, test_size=0.2, random_state=42)\n\n# X_train3, X_val3, y_train3, y_val3 = train_test_split(\n#     np.stack(museum['features'].values), museum['sentiment'].values, test_size=0.2, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Combine the training data\n# X_train = np.concatenate((X_train1, X_train2, X_train3), axis=0)\n# y_train = np.concatenate((y_train1, y_train2, y_train3), axis=0)\n\n# # Assign weights to the datasets\n# weights = np.concatenate((np.full(len(X_train1), 0.3), np.full(len(X_train2), 0.3), np.full(len(X_train3), 0.4)))\n\n# # Train the logistic regression model\n# model = LogisticRegression(max_iter=1000)\n# model.fit(X_train, y_train, sample_weight=weights)\n\n# # Combine the validation data\n# X_val = np.concatenate((X_val1, X_val2, X_val3), axis=0)\n# y_val = np.concatenate((y_val1, y_val2, y_val3), axis=0)\n\n# # Evaluate the model\n# y_pred = model.predict(X_val)\n# accuracy = accuracy_score(y_val, y_pred)\n# print(f'Validation Accuracy: {accuracy}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Print accuracy\n# print(f'Validation Accuracy: {accuracy}')\n# print(classification_report(y_val, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}